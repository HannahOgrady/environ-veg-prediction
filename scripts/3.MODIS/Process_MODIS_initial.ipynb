{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995b8e05-d82b-4ce5-a2f6-821416dd1686",
   "metadata": {},
   "source": [
    "# Formatting MODIS raw .hdf files\n",
    "\n",
    "This script loads in the .hdf files with MODIS land cover type data downloaded from NASA EarthData repository on 24 October 2024. I decided to use Python because the method for processing the data in Python is much more straightforward than for R. One the data is in array format, I save it and transfer these formatted data to R for the rest of the analysis using the pickle package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c35767-75ee-42e2-982b-19bf244ba720",
   "metadata": {},
   "source": [
    "First, I am importing the packages that are necessary for this pipeline, notable the xarray package is what processes the .hdf file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d964846b-90bb-450d-80f4-8767dd40bd21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import warnings\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray as rxr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad226e9-0edd-4733-8493-1e4998a1803d",
   "metadata": {},
   "source": [
    "Next, I am defining the datasets that are necessary to extract from each .hdf file. These are as follows:\n",
    "\n",
    "* LC_Type1: annual IGBP classification\n",
    "* LC_Type2: annual UMD classification\n",
    "* LC_Type3: annual LAI classification\n",
    "* QC: Product quality flags\n",
    "* LW: Binary land (class 2)/water (class 1) mask derived from MOD44W\n",
    "\n",
    "The follow datasets are dropped:\n",
    "\n",
    "* LC_Type4: annual BGC classification, because it does not include savanna or grassland\n",
    "* LC_Type5: annual PFT classification, because it does not incldue savanna or grassland\n",
    "* LC_Prop1: LCCS1 land cover layer, because it does not include savanna or cropland (so converting plant types to ecosystems would be impossible if we don't know what corresponds to cropland)\n",
    "* LC_Prop2: LCCS2 land use layer, because it does not include savanna or prairie\n",
    "* LC_Prop3: LCCS3 surface hydrology layer, because it does not include savanna\n",
    "* LC_Prop1_Ass: LCCS1 land cover layer confidence, because I'm not using LC_Prop1\n",
    "* LC_Prop2_Ass: LCCS2 land use layer confidence, because I'm not using LC_Prop2\n",
    "* LC_Prop3_Ass: LCCS3 surface hydrology layer confidence, because I'm not using LC_Prop3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a696ce-40ea-43b4-ae24-482f7933b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of datasets we want to keep from the data\n",
    "desired_datasets = ['LC_Type1',\n",
    "                    'LC_Type2',\n",
    "                    'LC_Type3',\n",
    "                    'QC',\n",
    "                    'LW']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e855c-8200-482f-8130-4f9d8dff755e",
   "metadata": {},
   "source": [
    "Now, I'm defining a function that we can loop over for formatting the data, including loading in the desired datasets, reprojecting the data, and converting it to an array format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce473f23-317d-44ca-819e-6d42f54c41d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for formatting data\n",
    "def data_func(i):\n",
    "    src = rxr.open_rasterio(file_list[i],\n",
    "                            masked = True,\n",
    "                            variable = desired_datasets).squeeze()\n",
    "    src_proj = src.rio.reproject('EPSG:3175')\n",
    "    src_array = xr.Dataset.to_array(src_proj)\n",
    "    return(src_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebf8f69-78ba-41a4-9c1d-f5615bc4418a",
   "metadata": {},
   "source": [
    "## Cell 1: h10v04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26af56-b1e6-4385-a309-0bff4996719a",
   "metadata": {},
   "source": [
    "Because each cell has different coordinates, we need to process the data for each cell separately. Therefore, I previously divided all the downloaded datasets into five cell subdirectories manually. That is, I used the download script included in the MODIS/ directory to dump all files in the MODIS directory, then I moved the files into different subdirectories based on their horizontal and vertical tile numbers.\n",
    "\n",
    "Here, we start with cell h10v04."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7622e01f-45d8-48f3-856f-5bf1c6a834d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining directory and data files for first cell\n",
    "data_dir = '/Volumes/FileBackup/SDM_bigdata/MODIS/h10v04'\n",
    "# use pathlib.path\n",
    "file_list = glob(os.path.join(data_dir, '*.hdf'))\n",
    "\n",
    "# Vector of file index for this cell\n",
    "ii = np.arange(np.shape(file_list)[0])\n",
    "ii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ead6321-692f-442b-ab35-858d74b5fa4a",
   "metadata": {},
   "source": [
    "Now, I use the function I defined above to loop over all the individual .hdf files for each year's data product in the same cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02f4e0d8-1b3c-4a8b-83cb-28b343e49970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all files in this cell and put into list format\n",
    "list_h10v04 = [data_func(i) for i in ii]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ffd5d-87b4-4d7b-b3ca-79fed3d84416",
   "metadata": {},
   "source": [
    "Since all the files have the same format, we can convert it to an array, which allows us to manipulate it in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d91dbb16-56dc-46f0-9f4d-16f13501023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "array_h10v04 = np.array(list_h10v04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3f03b1-4bf8-415c-9869-42c055ca6969",
   "metadata": {},
   "source": [
    "Here, I'm checking the dimensions to make sure it makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0afc92ab-240a-48b2-85ab-2a09f8a59bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 5, 2201, 2584)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape\n",
    "# 23 = years\n",
    "# 5 = datasets\n",
    "# 2201 x 2584 = coordinates\n",
    "np.shape(array_h10v04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd7f4cb-9a97-4810-9f52-352b3db3dda7",
   "metadata": {},
   "source": [
    "Now, I'm extracting the coordinates for two samples of the files. I make sure that they're identical before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35afaaf0-be5b-4e40-b8df-cdd09551bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract latitude and longitude\n",
    "samp1 = rxr.open_rasterio(file_list[0], masked = True, variable = desired_datasets).squeeze()\n",
    "samp2 = rxr.open_rasterio(file_list[10], masked = True, variable = desired_datasets).squeeze()\n",
    "\n",
    "samp1_reprojected = samp1.rio.reproject('EPSG:3175')\n",
    "samp2_reprojected = samp2.rio.reproject('EPSG:3175')\n",
    "\n",
    "lat_1 = samp1_reprojected['y']\n",
    "lat_2 = samp2_reprojected['y']\n",
    "lon_1 = samp1_reprojected['x']\n",
    "lon_2 = samp2_reprojected['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee3620dd-3bbe-4d13-b8f6-52dc09cfa783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "2201\n",
      "2584\n"
     ]
    }
   ],
   "source": [
    "# Check to make sure that the lat & lons are the same \n",
    "print(np.array_equal(lat_1, lat_2)) # should be True\n",
    "print(np.array_equal(lon_1, lon_2)) # should be True\n",
    "\n",
    "print(len(lat_1))\n",
    "print(len(lon_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8907b9ef-dc92-4be5-80ac-1c9750cad599",
   "metadata": {},
   "source": [
    "Finally, I format and save the coordinates and datasets for this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06283b3f-e940-412e-b08c-a9619b471aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objects to save\n",
    "lats = np.array(lat_1)\n",
    "lons = np.array(lon_1)\n",
    "modis_h10v04 = array_h10v04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8754f19d-3392-469c-b91e-473d7c97cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in pickle files\n",
    "filehandler = open('../../data/intermediate/MODIS/lats_h10v04.pickle', 'wb')\n",
    "pickle.dump(lats, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open('../../data/intermediate/MODIS/lons_h10v04.pickle', 'wb')\n",
    "pickle.dump(lons, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open('../../data/intermediate/MODIS/modis_h10v04.pickle', 'wb')\n",
    "pickle.dump(modis_h10v04, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd18ff94-9544-4545-a37e-48ad71558fa3",
   "metadata": {},
   "source": [
    "## Cell 2: h10v05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c46cd7-c269-45c8-a18d-61f306c70e21",
   "metadata": {},
   "source": [
    "Now that I've run through things once, I'm not going to explain the invidiual steps. I'll just repeat them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc053a8e-8c42-400b-8d0e-f5d4b6c8ecd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/Volumes/FileBackup/SDM_bigdata/MODIS/h10v05'\n",
    "file_list = glob(os.path.join(data_dir, '*.hdf'))\n",
    "\n",
    "ii = np.arange(np.shape(file_list)[0])\n",
    "ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80e48481-65e4-45ec-824c-4e0bca0e67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_h10v05 = [data_func(i) for i in ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d332b83-a7b7-417e-8faf-f195476a5fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_h10v05 = np.array(list_h10v05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c26ad575-bd57-414a-b6bb-b9c9addb5d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 5, 1862, 2841)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(array_h10v05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df6b117f-05e1-48f6-a967-871afca23a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp1 = rxr.open_rasterio(file_list[0], masked = True, variable = desired_datasets).squeeze()\n",
    "samp2 = rxr.open_rasterio(file_list[10], masked = True, variable = desired_datasets).squeeze()\n",
    "\n",
    "samp1_reprojected = samp1.rio.reproject('EPSG:3175')\n",
    "samp2_reprojected = samp2.rio.reproject('EPSG:3175')\n",
    "\n",
    "lat_1 = samp1_reprojected['y']\n",
    "lat_2 = samp2_reprojected['y']\n",
    "lon_1 = samp1_reprojected['x']\n",
    "lon_2 = samp2_reprojected['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "345229ef-0484-4a9e-b4ec-d3c2a633c112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "1862\n",
      "2841\n"
     ]
    }
   ],
   "source": [
    "print(np.array_equal(lat_1, lat_2))\n",
    "print(np.array_equal(lon_1, lon_2))\n",
    "\n",
    "print(len(lat_1))\n",
    "print(len(lon_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7970d7e-80b7-4b92-82b6-0a39d3db8b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = np.array(lat_1)\n",
    "lons = np.array(lon_1)\n",
    "modis_h10v05 = array_h10v05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e427eec8-c874-41c8-b5d5-782f590ad24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open('../../data/intermediate/MODIS/lats_h10v05.pickle', 'wb')\n",
    "pickle.dump(lats, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open('../../data/intermediate/MODIS/lons_h10v05.pickle', 'wb')\n",
    "pickle.dump(lons, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open('../../data/intermediate/MODIS/modis_h10v05.pickle', 'wb')\n",
    "pickle.dump(modis_h10v05, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab963547-eee4-4789-b741-f87d84edb931",
   "metadata": {},
   "source": [
    "## Cell 3: h11v04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b69dbca2-c00a-4aa4-9ecc-e86c3b47ca14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/Volumes/FileBackup/SDM_bigdata/MODIS/h11v04'\n",
    "file_list = glob(os.path.join(data_dir, '*.hdf'))\n",
    "\n",
    "ii = np.arange(np.shape(file_list)[0])\n",
    "ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2135d1a8-8d43-4c17-9454-0611fb77bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_h11v04 = [data_func(i) for i in ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df692008-ed64-41f5-8670-fa88f3503bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_h11v04 = np.array(list_h11v04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "697456fe-8d80-4629-a0cc-b6fd29c446da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 5, 1811, 2881)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(array_h11v04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bd692bb-1acd-4cb6-bc06-8c633d56b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp1 = rxr.open_rasterio(file_list[0], masked = True, variable = desired_datasets).squeeze()\n",
    "samp2 = rxr.open_rasterio(file_list[10], masked = True, variable = desired_datasets).squeeze()\n",
    "\n",
    "samp1_reprojected = samp1.rio.reproject('EPSG:3175')\n",
    "samp2_reprojected = samp2.rio.reproject('EPSG:3175')\n",
    "\n",
    "lat_1 = samp1_reprojected['y']\n",
    "lat_2 = samp2_reprojected['y']\n",
    "lon_1 = samp1_reprojected['x']\n",
    "lon_2 = samp2_reprojected['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1df9202-513d-4771-911a-896675bae2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "1811\n",
      "2881\n"
     ]
    }
   ],
   "source": [
    "print(np.array_equal(lat_1, lat_2))\n",
    "print(np.array_equal(lon_1, lon_2))\n",
    "\n",
    "print(len(lat_1))\n",
    "print(len(lon_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20ac3de8-da13-46e2-8a86-3b46a6e8221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = np.array(lat_1)\n",
    "lons = np.array(lon_1)\n",
    "modis_h11v04 = array_h11v04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1941f3f5-4c1e-432d-9620-7114fc394d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open('../../data/intermediate/MODIS/lats_h11v04.pickle', 'wb')\n",
    "pickle.dump(lats, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open('../../data/intermediate/MODIS/lons_h11v04.pickle', 'wb')\n",
    "pickle.dump(lons, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open('../../data/intermediate/MODIS/modis_h11v04.pickle', 'wb')\n",
    "pickle.dump(modis_h11v04, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090391d7-4546-4498-89ca-6c8bd4380fc6",
   "metadata": {},
   "source": [
    "## Cell 4: h11v05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbe9d48b-86dc-4a73-8a25-161c2775fbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/Volumes/FileBackup/SDM_bigdata/MODIS/h11v05'\n",
    "file_list = glob(os.path.join(data_dir, '*.hdf'))\n",
    "\n",
    "ii = np.arange(np.shape(file_list)[0])\n",
    "ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "486facfd-2527-4486-9ed0-5f0fb78d8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_h11v05 = [data_func(i) for i in ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dd94c0a-db2c-4010-85b1-e3970ee121bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_h11v05 = np.array(list_h11v05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59819c2e-dd90-4e3a-b848-ce485252151f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 5, 1657, 3052)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(array_h11v05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9471c2e9-5a3a-41f2-b9fa-b9a0c5c89a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp1 = rxr.open_rasterio(file_list[0], masked = True, variable = desired_datasets).squeeze()\n",
    "samp2 = rxr.open_rasterio(file_list[10], masked = True, variable = desired_datasets).squeeze()\n",
    "\n",
    "samp1_reprojected = samp1.rio.reproject('EPSG:3175')\n",
    "samp2_reprojected = samp2.rio.reproject('EPSG:3175')\n",
    "\n",
    "lat_1 = samp1_reprojected['y']\n",
    "lat_2 = samp2_reprojected['y']\n",
    "lon_1 = samp1_reprojected['x']\n",
    "lon_2 = samp2_reprojected['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97b1b2e3-db5b-426c-bd9a-2c1023a91fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "1657\n",
      "3052\n"
     ]
    }
   ],
   "source": [
    "print(np.array_equal(lat_1, lat_2))\n",
    "print(np.array_equal(lon_1, lon_2))\n",
    "\n",
    "print(len(lat_1))\n",
    "print(len(lon_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfed84d6-135e-4e17-b46a-4f71801f6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = np.array(lat_1)\n",
    "lons = np.array(lon_1)\n",
    "modis_h11v05 = array_h11v05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53cf1103-dfa7-49d5-b027-b1915d302086",
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open('../../data/intermediate/MODIS/lats_h11v05.pickle', 'wb')\n",
    "pickle.dump(lats, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open('../../data/intermediate/MODIS/lons_h11v05.pickle', 'wb')\n",
    "pickle.dump(lons, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open('../../data/intermediate/MODIS/modis_h11v05.pickle', 'wb')\n",
    "pickle.dump(modis_h11v05, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5018753a-d65f-4267-bcb6-087fb07713d6",
   "metadata": {},
   "source": [
    "## Cell 5: h12v04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9204a866-7f53-4877-b18d-ddb1968a43c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/Volumes/FileBackup/SDM_bigdata/MODIS/h12v04'\n",
    "file_list = glob(os.path.join(data_dir, '*.hdf'))\n",
    "\n",
    "ii = np.arange(np.shape(file_list)[0])\n",
    "ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f2e8ef8-431d-4314-ac5c-2d8a3baa072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_h12v04 = [data_func(i) for i in ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "891d6a02-46d0-4b80-8128-d5e541e54e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_h12v04 = np.array(list_h12v04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc71522e-e532-4f2d-9c2b-ba2f391ff179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 5, 1582, 3109)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(array_h12v04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9da964c7-0be1-46d1-9e89-4a31dc8efa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp1 = rxr.open_rasterio(file_list[0], masked = True, variable = desired_datasets).squeeze()\n",
    "samp2 = rxr.open_rasterio(file_list[10], masked = True, variable = desired_datasets).squeeze()\n",
    "\n",
    "samp1_reprojected = samp1.rio.reproject('EPSG:3175')\n",
    "samp2_reprojected = samp2.rio.reproject('EPSG:3175')\n",
    "\n",
    "lat_1 = samp1_reprojected['y']\n",
    "lat_2 = samp2_reprojected['y']\n",
    "lon_1 = samp1_reprojected['x']\n",
    "lon_2 = samp2_reprojected['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a5f3bc04-9885-4d07-ad5f-e1b102508141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "1582\n",
      "3109\n"
     ]
    }
   ],
   "source": [
    "print(np.array_equal(lat_1, lat_2))\n",
    "print(np.array_equal(lon_1, lon_2))\n",
    "\n",
    "print(len(lat_1))\n",
    "print(len(lon_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7738eaf0-e818-4221-b8da-e6ea219d5a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = np.array(lat_1)\n",
    "lons = np.array(lon_1)\n",
    "modis_h12v04 = array_h12v04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50d24379-97f3-40e7-8018-107261daca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open('../../data/intermediate/MODIS/lats_h12v04.pickle', 'wb')\n",
    "pickle.dump(lats, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open('../../data/intermediate/MODIS/lons_h12v04.pickle', 'wb')\n",
    "pickle.dump(lats, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open('../../data/intermediate/MODIS/modis_h12v04.pickle', 'wb')\n",
    "pickle.dump(modis_h12v04, filehandler)\n",
    "filehandler.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLenv]",
   "language": "python",
   "name": "conda-env-MLenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
