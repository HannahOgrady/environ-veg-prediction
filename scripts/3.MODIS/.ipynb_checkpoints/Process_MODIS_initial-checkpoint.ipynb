{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995b8e05-d82b-4ce5-a2f6-821416dd1686",
   "metadata": {},
   "source": [
    "# Formatting MODIS raw .hdf files\n",
    "\n",
    "This script loads in the .hdf files with MODIS land cover type data downloaded from NASA EarthData repository on 24 October 2024. I decided to use Python because the method for processing the data in Python is much more straightforward than for R. One the data is in array format, I save it and transfer these formatted data to R for the rest of the analysis using the pickle package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c35767-75ee-42e2-982b-19bf244ba720",
   "metadata": {},
   "source": [
    "First, I am importing the packages that are necessary for this pipeline, notable the xarray package is what processes the .hdf file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d964846b-90bb-450d-80f4-8767dd40bd21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import warnings\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray as rxr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad226e9-0edd-4733-8493-1e4998a1803d",
   "metadata": {},
   "source": [
    "Next, I am defining the datasets that are necessary to extract from each .hdf file. These are as follows:\n",
    "\n",
    "* LC_Type1: annual IGBP classification\n",
    "* LC_Type2: annual UMD classification\n",
    "* LC_Type3: annual LAI classification\n",
    "* QC: Product quality flags\n",
    "* LW: Binary land (class 2)/water (class 1) mask derived from MOD44W\n",
    "\n",
    "The follow datasets are dropped:\n",
    "\n",
    "* LC_Type4: annual BGC classification, because it does not include savanna or grassland\n",
    "* LC_Type5: annual PFT classification, because it does not incldue savanna or grassland\n",
    "* LC_Prop1: LCCS1 land cover layer, because it does not include savanna or cropland (so converting plant types to ecosystems would be impossible if we don't know what corresponds to cropland)\n",
    "* LC_Prop2: LCCS2 land use layer, because it does not include savanna or prairie\n",
    "* LC_Prop3: LCCS3 surface hydrology layer, because it does not include savanna\n",
    "* LC_Prop1_Ass: LCCS1 land cover layer confidence, because I'm not using LC_Prop1\n",
    "* LC_Prop2_Ass: LCCS2 land use layer confidence, because I'm not using LC_Prop2\n",
    "* LC_Prop3_Ass: LCCS3 surface hydrology layer confidence, because I'm not using LC_Prop3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a696ce-40ea-43b4-ae24-482f7933b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of datasets we want to keep from the data\n",
    "desired_datasets = ['LC_Type1',\n",
    "                    'LC_Type2',\n",
    "                    'LC_Type3',\n",
    "                    'QC',\n",
    "                    'LW']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e855c-8200-482f-8130-4f9d8dff755e",
   "metadata": {},
   "source": [
    "Now, I'm defining a function that we can loop over for formatting the data, including loading in the desired datasets, reprojecting the data, and converting it to an array format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce473f23-317d-44ca-819e-6d42f54c41d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for formatting data\n",
    "def data_func(i):\n",
    "    src = rxr.open_rasterio(file_list[i],\n",
    "                            masked = True,\n",
    "                            variable = desired_datasets).squeeze()\n",
    "    src_proj = src.rio.reproject('EPSG:3175')\n",
    "    src_array = xr.Dataset.to_array(src_proj)\n",
    "    return(src_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebf8f69-78ba-41a4-9c1d-f5615bc4418a",
   "metadata": {},
   "source": [
    "## Cell 1: h10v04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26af56-b1e6-4385-a309-0bff4996719a",
   "metadata": {},
   "source": [
    "Because each cell has different coordinates, we need to process the data for each cell separately. Therefore, I previously divided all the downloaded datasets into five cell subdirectories manually. That is, I used the download script included in the MODIS/ directory to dump all files in the MODIS directory, then I moved the files into different subdirectories based on their horizontal and vertical tile numbers.\n",
    "\n",
    "Here, we start with cell h10v04."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7622e01f-45d8-48f3-856f-5bf1c6a834d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining directory and data files for first cell\n",
    "data_dir = '/Volumes/FileBackup/SDM_bigdata/MODIS/h10v04'\n",
    "# use pathlib.path\n",
    "file_list = glob(os.path.join(data_dir, '*.hdf'))\n",
    "\n",
    "# Vector of file index for this cell\n",
    "ii = np.arange(np.shape(file_list)[0])\n",
    "ii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ead6321-692f-442b-ab35-858d74b5fa4a",
   "metadata": {},
   "source": [
    "Now, I use the function I defined above to loop over all the individual .hdf files for each year's data product in the same cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f4e0d8-1b3c-4a8b-83cb-28b343e49970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all files in this cell and put into list format\n",
    "list_h10v04 = [data_func(i) for i in ii]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ffd5d-87b4-4d7b-b3ca-79fed3d84416",
   "metadata": {},
   "source": [
    "Since all the files have the same format, we can convert it to an array, which allows us to manipulate it in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d91dbb16-56dc-46f0-9f4d-16f13501023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "array_h10v04 = np.array(list_h10v04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3f03b1-4bf8-415c-9869-42c055ca6969",
   "metadata": {},
   "source": [
    "Here, I'm checking the dimensions to make sure it makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0afc92ab-240a-48b2-85ab-2a09f8a59bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 5, 2201, 2584)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape\n",
    "# 23 = years\n",
    "# 5 = datasets\n",
    "# 2201 x 2584 = coordinates\n",
    "np.shape(array_h10v04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd7f4cb-9a97-4810-9f52-352b3db3dda7",
   "metadata": {},
   "source": [
    "Now, I'm extracting the coordinates for two samples of the files. I make sure that they're identical before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35afaaf0-be5b-4e40-b8df-cdd09551bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract latitude and longitude\n",
    "samp1 = rxr.open_rasterio(file_list[0], masked = True, variable = desired_datasets).squeeze()\n",
    "samp2 = rxr.open_rasterio(file_list[10], masked = True, variable = desired_datasets).squeeze()\n",
    "\n",
    "samp1_reprojected = samp1.rio.reproject('EPSG:3175')\n",
    "samp2_reprojected = samp2.rio.reproject('EPSG:3175')\n",
    "\n",
    "lat_1 = samp1_reprojected['y']\n",
    "lat_2 = samp2_reprojected['y']\n",
    "lon_1 = samp1_reprojected['x']\n",
    "lon_2 = samp2_reprojected['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee3620dd-3bbe-4d13-b8f6-52dc09cfa783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "2201\n",
      "2584\n"
     ]
    }
   ],
   "source": [
    "# Check to make sure that the lat & lons are the same \n",
    "print(np.array_equal(lat_1, lat_2)) # should be True\n",
    "print(np.array_equal(lon_1, lon_2)) # should be True\n",
    "\n",
    "print(len(lat_1))\n",
    "print(len(lon_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8907b9ef-dc92-4be5-80ac-1c9750cad599",
   "metadata": {},
   "source": [
    "Finally, I format and save the coordinates and datasets for this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06283b3f-e940-412e-b08c-a9619b471aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objects to save\n",
    "lats = np.array(lat_1)\n",
    "lons = np.array(lon_1)\n",
    "modis_h10v04 = array_h10v04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8754f19d-3392-469c-b91e-473d7c97cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in pickle files\n",
    "filehandler = open('../../data/intermediate/MODIS/lats_h10v04.pickle', 'wb')\n",
    "pickle.dump(lats, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open('../../data/intermediate/MODIS/lons_h10v04.pickle', 'wb')\n",
    "pickle.dump(lons, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open('../../data/intermediate/MODIS/modis_h10v04.pickle', 'wb')\n",
    "pickle.dump(modis_h10v04, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd18ff94-9544-4545-a37e-48ad71558fa3",
   "metadata": {},
   "source": [
    "## Cell 2: h10v05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c46cd7-c269-45c8-a18d-61f306c70e21",
   "metadata": {},
   "source": [
    "Now that I've run through things once, I'm not going to explain the invidiual steps. I'll just repeat them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc053a8e-8c42-400b-8d0e-f5d4b6c8ecd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/Volumes/FileBackup/SDM_bigdata/MODIS/h10v05'\n",
    "file_list = glob(os.path.join(data_dir, '*.hdf'))\n",
    "\n",
    "ii = np.arange(np.shape(file_list)[0])\n",
    "ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80e48481-65e4-45ec-824c-4e0bca0e67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_h10v05 = [data_func(i) for i in ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d332b83-a7b7-417e-8faf-f195476a5fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_h10v05 = np.array(list_h10v05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c26ad575-bd57-414a-b6bb-b9c9addb5d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 5, 1862, 2841)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(array_h10v05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df6b117f-05e1-48f6-a967-871afca23a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp1 = rxr.open_rasterio(file_list[0], masked = True, variable = desired_datasets).squeeze()\n",
    "samp2 = rxr.open_rasterio(file_list[10], masked = True, variable = desired_datasets).squeeze()\n",
    "\n",
    "samp1_reprojected = samp1.rio.reproject('EPSG:3175')\n",
    "samp2_reprojected = samp2.rio.reproject('EPSG:3175')\n",
    "\n",
    "lat_1 = samp1_reprojected['y']\n",
    "lat_2 = samp2_reprojected['y']\n",
    "lon_1 = samp1_reprojected['x']\n",
    "lon_2 = samp2_reprojected['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "345229ef-0484-4a9e-b4ec-d3c2a633c112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "1862\n",
      "2841\n"
     ]
    }
   ],
   "source": [
    "print(np.array_equal(lat_1, lat_2))\n",
    "print(np.array_equal(lon_1, lon_2))\n",
    "\n",
    "print(len(lat_1))\n",
    "print(len(lon_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7970d7e-80b7-4b92-82b6-0a39d3db8b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = np.array(lat_1)\n",
    "lons = np.array(lon_1)\n",
    "modis_h10v05 = array_h10v05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e427eec8-c874-41c8-b5d5-782f590ad24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open('../../data/intermediate/MODIS/lats_h10v05.pickle', 'wb')\n",
    "pickle.dump(lats, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open('../../data/intermediate/MODIS/lons_h10v05.pickle', 'wb')\n",
    "pickle.dump(lons, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open('../../data/intermediate/MODIS/modis_h10v05.pickle', 'wb')\n",
    "pickle.dump(modis_h10v05, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf00f1f-a3de-438c-9192-e76ad894ea33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLenv]",
   "language": "python",
   "name": "conda-env-MLenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
